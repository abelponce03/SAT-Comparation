# ğŸ”¬ SAT Benchmark Suite

A comprehensive benchmarking and analysis platform for SAT solvers. Built with Python and Streamlit.

## âœ¨ Features

- **âš™ï¸ Solver Management**: Upload, auto-compile, and manage multiple SAT solvers
- **ğŸ“ Benchmark Management**: Auto-classify benchmarks by problem family with metadata extraction
- **ğŸš€ Experiment Execution**: Parallel execution with real-time monitoring
- **ğŸ“Š Results Management**: SQLite database for efficient result storage and querying
- **ğŸ“ˆ Statistical Analysis**: PAR-2, VBS, confidence intervals, and more
- **ğŸ“‰ Visualization**: Cactus plots, scatter plots, and heatmaps
- **ğŸ“„ Report Generation**: Automated PDF/HTML report generation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Installation

1. **Clone or navigate to the project directory:**
   ```bash
   cd sat-benchmark-suite
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the application:**
   ```bash
   streamlit run app/main.py
   ```

4. **Open your browser** to `http://localhost:8501`

## ğŸ“ Project Structure

```
sat-benchmark-suite/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                          # Main application page
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_âš™ï¸_Setup_Solvers.py       # Solver management
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ“_Manage_Benchmarks.py   # Benchmark management
â”‚   â”‚   â”œâ”€â”€ 3_ğŸš€_Run_Experiments.py     # Experiment execution
â”‚   â”‚   â”œâ”€â”€ 4_ğŸ“Š_View_Results.py        # Results viewer
â”‚   â”‚   â”œâ”€â”€ 5_ğŸ“ˆ_Statistical_Analysis.py # Statistical tools
â”‚   â”‚   â”œâ”€â”€ 6_ğŸ“‰_Visualizations.py      # Plotting tools
â”‚   â”‚   â””â”€â”€ 7_ğŸ“„_Reports.py             # Report generator
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ database.py                  # SQLite manager
â”‚   â”‚   â”œâ”€â”€ solver_manager.py            # Solver operations
â”‚   â”‚   â”œâ”€â”€ benchmark_manager.py         # Benchmark operations
â”‚   â”‚   â”œâ”€â”€ executor.py                  # Experiment executor
â”‚   â”‚   â””â”€â”€ monitor.py                   # Real-time monitoring
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ statistics.py                # Statistical analysis
â”‚   â”‚   â””â”€â”€ plots.py                     # Plotting functions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ cnf_parser.py                # CNF file parser
â”‚       â”œâ”€â”€ solver_detector.py           # Auto-detect solvers
â”‚       â””â”€â”€ helpers.py                   # Helper functions
â”œâ”€â”€ solvers/                             # Your solvers go here
â”œâ”€â”€ benchmarks/                          # Your CNF files go here
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ experiments.db                   # SQLite database
â”‚   â””â”€â”€ exports/                         # Exported CSVs
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ app_config.yaml                  # App configuration
â”‚   â””â”€â”€ solver_templates.json            # Solver templates
â””â”€â”€ requirements.txt                     # Python dependencies
```

## ğŸ“– Usage Guide

### 1. Setup Solvers

Navigate to **âš™ï¸ Setup Solvers** page:

- **Upload Archive**: Upload ZIP/TAR.GZ of solver source code
- **Auto-compilation**: System auto-detects Makefile and compiles
- **Manual configuration**: Specify custom build commands
- **Pre-compiled**: Add already compiled solvers

**Supported Solvers:**
- MiniSat
- CaDiCaL
- Glucose
- CryptoMiniSat
- Kissat
- Lingeling
- Any custom solver

### 2. Add Benchmarks

Navigate to **ğŸ“ Manage Benchmarks** page:

- **Scan Directory**: Auto-discover CNF files
- **Upload**: Upload individual or multiple benchmarks
- **Auto-classification**: System classifies by problem family
- **Metadata Extraction**: Automatically extracts variables, clauses, ratio

### 3. Run Experiments

Navigate to **ğŸš€ Run Experiments** page:

1. **Create Experiment**: Give it a name and description
2. **Select Solvers**: Choose which solvers to benchmark
3. **Select Benchmarks**: Filter and select benchmarks
4. **Configure**: Set timeout, memory limit, parallel jobs
5. **Launch**: Monitor progress in real-time

### 4. View Results

Navigate to **ğŸ“Š View Results** page:

- **Filter**: By solver, benchmark family, result status
- **Export**: Download as CSV or Excel
- **Details**: View complete run information
- **Compare**: Side-by-side comparison

### 5. Statistical Analysis

Navigate to **ğŸ“ˆ Statistical Analysis** page:

- **PAR-2 Scoring**: Penalized average runtime
- **Virtual Best Solver (VBS)**: Best possible performance
- **Pairwise Comparisons**: Statistical significance tests
- **Confidence Intervals**: Bootstrap or t-distribution

### 6. Visualizations

Navigate to **ğŸ“‰ Visualizations** page:

- **Cactus Plot**: Solved instances over time
- **Scatter Plot**: Solver A vs Solver B runtime
- **Heatmap**: Result matrix
- **Performance Profile**: Cumulative distribution

### 7. Generate Reports

Navigate to **ğŸ“„ Reports** page:

- **PDF Reports**: Publication-ready documents
- **HTML Reports**: Interactive web pages
- **Custom Templates**: Customize report structure
- **Include Plots**: Embed all visualizations

## âš™ï¸ Configuration

### Database Configuration

The SQLite database stores:
- **Solvers**: Name, version, executable path, compilation info
- **Benchmarks**: Metadata, classification, checksums
- **Experiments**: Configuration, status, timing
- **Runs**: Complete results with 40+ metrics per run

### Solver Templates

Edit `config/solver_templates.json` to add templates for new solvers:

```json
{
  "your_solver": {
    "name": "Your Solver",
    "build_files": ["Makefile"],
    "build_commands": ["make"],
    "executable_patterns": ["build/your_solver"],
    "test_command": "{executable} --version"
  }
}
```

### Application Settings

Edit `config/app_config.yaml`:

```yaml
defaults:
  timeout_seconds: 5000
  memory_limit_mb: 8192
  parallel_jobs: 4

benchmark_families:
  - name: "custom"
    pattern: "custom_.*"
    description: "Custom Problems"
```

## ğŸ“Š Metrics Collected

For each run, the system collects:

### Basic Metrics
- Result (SAT/UNSAT/TIMEOUT/MEMOUT/ERROR)
- CPU time, wall time, system time
- Memory usage (max, average)
- Exit code

### System Metrics
- Page faults
- Context switches
- CPU percentage

### Solver Statistics (if available)
- Conflicts, decisions, propagations
- Restarts
- Learnt literals and clauses
- Decision heights
- And more...

### Computed Metrics
- PAR-2 score (2Ã— timeout for unsolved)
- Clause/variable ratio
- Benchmark difficulty classification

## ğŸ”§ Advanced Usage

### Custom Metrics

To add custom metrics to your runs, edit `app/core/database.py` and add columns to the `runs` table.

### Custom Parsers

Add solver-specific output parsers in `app/utils/solver_detector.py` to extract additional metrics.

### Parallel Execution

Adjust `parallel_jobs` in experiment configuration:
- `1`: Sequential execution
- `> 1`: Parallel execution with multiprocessing
- Recommended: Number of CPU cores - 1

### Checkpointing

Experiments automatically checkpoint every 100 runs. To resume:
1. Go to experiment page
2. Click "Resume" on interrupted experiment

## ğŸ› Troubleshooting

### Solver won't compile
- Check build dependencies (gcc, make, cmake, etc.)
- Review compilation logs in Setup page
- Try manual compilation first, then add as pre-compiled

### Benchmark not loading
- Ensure file is valid CNF format
- Check file permissions
- Look for "p cnf" line in file header

### Database errors
- Check `results/` directory permissions
- Delete `experiments.db` to recreate (loses data!)
- Check SQLite is accessible

### Out of memory during experiments
- Reduce `parallel_jobs`
- Lower `memory_limit_mb`
- Filter benchmarks to smaller instances

## ğŸ“ Best Practices

1. **Start Small**: Test with 10-20 benchmarks before large experiments
2. **Document**: Add descriptions to experiments for future reference
3. **Backup**: Export results regularly
4. **Monitor**: Watch system resources during experiments
5. **Validate**: Test solvers individually before batch runs
6. **Compare Incrementally**: Add one solver at a time

## ğŸ¤ Contributing

This is a research tool. Contributions welcome:
- Add new solver templates
- Improve statistical analysis
- Add visualization types
- Optimize database queries

## ğŸ“„ License

For academic and research use.

## ğŸ™ Acknowledgments

Built for SAT solver research and benchmarking. Supports standard SAT competition formats.

## ğŸ“§ Support

For issues and questions, refer to the in-app FAQ or check the logs in the console.

---

**Happy Benchmarking! ğŸš€**
